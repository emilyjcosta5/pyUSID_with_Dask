{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Job Queue on Summit\n",
    "\n",
    "**Emily Costa**\n",
    "\n",
    "07/31/2019\n",
    "***\n",
    "\n",
    "This tutorial will use the Python packages, pyUSID and pyrcoscopy, as examples. As the parallel_compute() function in pyUSID does not scale up to multi-node machines, Dask_jobqueue can be used to automate computation to clusters and supercomputers. This tutorial uses Summit at Oak Ridge National Laboratory, but can be applied to HPC systems that use LFS files to submit and deploy jobs. \n",
    "\n",
    "It is strongly recommended that Jupyter Lab is used on the remote machine to facilitate the interactive session.\n",
    "\n",
    "**Note**: The contents of this tutorial can be run in an interactive session on Summit. Please open this Jupyter Lab from a Summit login node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Lab on Summit \n",
    "In order to properly complete and run this tutorial, follow these instructions to open Jupyter Lab on the remote machine, Summit.\n",
    "***\n",
    "**Open this Jupyter Notebook using Jupyter Lab on a Summit login node.**\n",
    "\n",
    "Note: If the repo containing this Jupyter Notebook does not exist in your current Summit path on the login node, it will automatically download. If it does, it will make sure the repo is up-to-date.\n",
    "***\n",
    "1. On summit login node, run this script and leave terminal running:\n",
    "> `source resources/jupyterlab.sh`\n",
    "\n",
    "2. Open a new terminal and run this on your local machine:\n",
    "> `ssh -L 8887:localhost:8887 user@login##.summit.olcf.ornl.gov`\n",
    "\n",
    "3. Now, open `localhost:8887` on your local browser and navigate to this Jupyter Notebook.\n",
    "***\n",
    "Now, let us begin the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import LFSCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pyUSID as usid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following format \n",
    "> `parameter: default     # Option flag and description`\n",
    "    \n",
    "    name: dask-worker           # -N option, name of job.\n",
    "\n",
    "***Dask worker options***\n",
    "    \n",
    "    cores: null                 # Total number of cores per job.\n",
    "    memory: null                # Total amount of memory per job.\n",
    "    processes: 1                # Number of Python processes per job\n",
    "\n",
    "    interface: null             # Network interface to use like eth0 or ib0\n",
    "    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler\n",
    "    local-directory: null       # Fast local storage location (/scratch, $TMPDIR)\n",
    "\n",
    "***LFS resource manager options***\n",
    "    \n",
    "    shebang: \"#!/usr/bin/env bash\"    # Command language\n",
    "    queue: null                       # -q option\n",
    "    project: null                     # -A option\n",
    "    walltime: '00:30:00'              # -l walltime: xx:xx:xx\n",
    "    extra: []\n",
    "    env-extra: []\n",
    "    resource-spec: null\n",
    "    job-extra: []\n",
    "    log-directory: null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accomodate an interactive session, let us add a few more commands. Create a list of additional commands then put it in job_extra parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_extra = list[]\n",
    "# Command to begin interactive session, which we will open in terminal later.\n",
    "job_extra.append('-Is')\n",
    "# The LDAP group list, cades-birthright in this case.\n",
    "job_extra.append('-W group_list=cades-birthright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up nodes and LFS script for requesting then running a job on Summit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory = '/.' # Current directory, feel free to change.\n",
    "\n",
    "cluster = PBSCluster(name='HelloWorld',\n",
    "                     project='birthright',\n",
    "                     resource_spec='nodes=2:ppn=32:qos=std',\n",
    "                     walltime='02:00:00',\n",
    "                     local_directory=local_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask-jobqueue Features\n",
    "Now, let us mess around with some of the features of dask-jobqueue.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=18, maximum=360) # Scale between 18 and 360 workers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
