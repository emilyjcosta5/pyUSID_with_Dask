{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Job Queue on Summit\n",
    "\n",
    "**Emily Costa**\n",
    "\n",
    "07/31/2019\n",
    "***\n",
    "\n",
    "This tutorial will use the Python packages, pyUSID and pyrcoscopy, as examples. As the parallel_compute() function in pyUSID does not scale up to multi-node machines, Dask_jobqueue can be used to automate computation to clusters and supercomputers. This tutorial uses Summit at Oak Ridge National Laboratory, but can be applied to HPC systems that use LFS files to submit and deploy jobs. \n",
    "\n",
    "It is strongly recommended that Jupyter Lab is used on the remote machine to facilitate the interactive session.\n",
    "\n",
    "**Note**: The contents of this tutorial can be run in an interactive session on Summit. Please open this Jupyter Lab from a Summit login node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Lab on Summit \n",
    "In order to properly complete and run this tutorial, follow these instructions to open Jupyter Lab on the remote machine, Summit.\n",
    "***\n",
    "**Open this Jupyter Notebook using Jupyter Lab on a Summit login node.**\n",
    "\n",
    "Note: If the full repo containing this Jupyter Notebook does not exist in your current Summit path on the login node, it will automatically download. If it does, it will make sure the repo is up-to-date.\n",
    "***\n",
    "1. On summit login node, run this script and leave terminal running:\n",
    "> `source /gpfs/alpine/gen011/scratch/ecost020/jupyterlab.sh`\n",
    "\n",
    "2. Open a new terminal and run this on your local machine:\n",
    "> `ssh -L 8887:localhost:8887 user@login##.summit.olcf.ornl.gov`\n",
    "\n",
    "3. Now, open `localhost:8887` on your local browser and navigate to this Jupyter Notebook.\n",
    "***\n",
    "Now, let us begin the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import LFSCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pyUSID as usid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following format \n",
    "> `parameter: default     # Option flag and description`\n",
    "    \n",
    "    name: dask-worker           # -N option, name of job.\n",
    "\n",
    "***Dask worker options***\n",
    "    \n",
    "    cores: null                 # Total number of cores per job.\n",
    "    memory: null                # Total amount of memory per job.\n",
    "    processes: 1                # Number of Python processes per job\n",
    "\n",
    "    interface: null             # Network interface to use like eth0 or ib0\n",
    "    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler\n",
    "    local-directory: null       # Fast local storage location (/scratch, $TMPDIR)\n",
    "\n",
    "***LFS resource manager options***\n",
    "    \n",
    "    shebang: \"#!/usr/bin/env bash\"    # Command language\n",
    "    queue: null                       # -q option\n",
    "    project: null                     # -A option\n",
    "    walltime: '00:30:00'              # -l walltime: xx:xx:xx\n",
    "    extra: []                         # Additional arguments to pass to `dask-worker`\n",
    "    env-extra: []                     # Other commands to add to script before launching worker.\n",
    "    job-extra: []                     # Additional commands\n",
    "    log-directory: null               # Directory to use for job scheduler logs.\n",
    "    lsf-units: null                   # Unit system for large units in resource usage set by the LSF_UNIT_FOR_LIMITS in the lsf.conf file of a cluster.\n",
    "    kwargs: null                      # Additional keyword arguments to pass to `LocalCluster`\n",
    "    \n",
    "***Summit Specific Flags***\n",
    "\n",
    "These can be added to job-extra, a list of additional flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accomodate an interactive session, let us add a few more commands. Create a list of additional commands then put it in job_extra parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_extra = list[]\n",
    "# Command to begin interactive session, which we will open in terminal later.\n",
    "job_extra.append('-Is')\n",
    "# Job name\n",
    "job_extra.append('-J dask-jq')\n",
    "# Compute specs\n",
    "job_extra.append('-nnodes2')    \n",
    "# Local SSD storage request\n",
    "# Hardware threat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up nodes and LFS script for requesting then running a job on Summit.\n",
    "\n",
    "The job request will mimick the following one:\n",
    "> `bsub -P project_name -J job_name -W 1:30 -nnodes 2 -B -alloc_flags nvme -Is /bin/bash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory = '/.' # Current directory, feel free to change.\n",
    "\n",
    "cluster = LSFCluster(name='HelloWorld',\n",
    "                     project='gen011',\n",
    "                     walltime='0:01:00',\n",
    "                     job-extra=job_extra\n",
    "                     local-directory=local_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask-jobqueue Scaling\n",
    "Now, let us mess around with some of the features of dask-jobqueue.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(2) # Number of workers, or jobs, you wish to schedule\n",
    "# or\n",
    "# Talk about later?: cluster.adapt(minimum=18, maximum=360) # Scale between 18 and 360 workers or 1 and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the Client class, which will be how you interact with the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the rest of the code will resemble daskâ€™s local schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
