{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dask Job Queue with pyUSID***\n",
    "\n",
    "Emily Costa\n",
    "\n",
    "07/31/2019\n",
    "\n",
    "As the parallel_compute() function in pyUSID does not scale up to multi-node machines, Dask_jobqueue can be used to automate computation to clusters and supercomputers. This tutorial uses the SHPC Condo at Oak Ridge National Laboratory, but can be applied to HPC systems that use PBS files to submit and deploy jobs. \n",
    "\n",
    "It is strongly recommended that Jupyter Notebook is used on the remote machine to facilitate the interactive session.\n",
    "\n",
    "The contents of this tutorial can be run in an interactive session on the SHPC Condo. Please open this Jupyter Notebook from Condo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "import pyUSID as usid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask\n",
    "    \n",
    "    name: dask-worker           # -N option, name of job.\n",
    "\n",
    "***Dask worker options***\n",
    "    \n",
    "    cores: null                 # Total number of cores per job.\n",
    "    memory: null                # Total amount of memory per job.\n",
    "    processes: 1                # Number of Python processes per job\n",
    "\n",
    "    interface: null             # Network interface to use like eth0 or ib0\n",
    "    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler\n",
    "    local-directory: null       # Fast local storage location (/scratch, $TMPDIR)\n",
    "\n",
    "***PBS resource manager options***\n",
    "    \n",
    "    shebang: \"#!/usr/bin/env bash\"    # Command language\n",
    "    queue: null                       # -q option\n",
    "    project: null                     # -A option\n",
    "    walltime: '00:30:00'              # -l walltime: xx:xx:xx\n",
    "    extra: []\n",
    "    env-extra: []\n",
    "    resource-spec: null\n",
    "    job-extra: []\n",
    "    log-directory: null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CADES SHPC Condo requires a few more commands, so add the to a list then put in job_extra parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_extra = list[]\n",
    "job_extra.append('-M eju@ornl.gov')\n",
    "job_extra.append('-W group_list=cades-birthright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up cluster and PBS script for running a batch job on SHPC Condo allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = PBSCluster(name='BayesianInference',\n",
    "                     project='birthright',\n",
    "                     resource_spec='nodes=2:ppn=32:qos=std',\n",
    "                     walltime='02:00:00',\n",
    "                     local_directory='$TMPDIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=18, maximum=360) # Scale between 18 and 360 workers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
