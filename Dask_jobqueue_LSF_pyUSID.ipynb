{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dask Job Queue with pyUSID***\n",
    "\n",
    "Emily Costa\n",
    "\n",
    "07/31/2019\n",
    "\n",
    "As the parallel_compute() function in pyUSID does not scale up to multi-node machines, Dask_jobqueue can be used to automate computation to clusters and supercomputers. This tutorial uses the SHPC Condo at Oak Ridge National Laboratory, but can be applied to HPC systems that use PBS files to submit and deploy jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = PBSCluster(cores=36,\n",
    "                     processes=18,\n",
    "                     memory=\"108GB\",\n",
    "                     project='gen011',\n",
    "                     queue='premium',\n",
    "                     resource_spec='select=1:ncpus=36:mem=109G',\n",
    "                     walltime='02:00:00',\n",
    "                     interface='ib0',\n",
    "                     local_directory='$TMPDIR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
